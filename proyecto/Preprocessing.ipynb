{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGAR LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date      Time                                         Tweet_Text  Type Media_Type   Hashtags   Tweet_Id                                          Tweet_Url  twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  Unnamed: 11\n",
      "0  16-11-11  15:26:37  Today we express our deepest gratitude to all ...  text      photo  ThankAVet  7.970e+17  https://twitter.com/realDonaldTrump/status/797...                                     127213     41112          NaN          NaN\n",
      "1  16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...  text        NaN        NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/797...                                     141527     28654          NaN          NaN\n",
      "2  16-11-11  11:14:20  Love the fact that the small groups of protest...  text        NaN        NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/797...                                     183729     50039          NaN          NaN\n",
      "3  16-11-11   2:19:44  Just had a very open and successful presidenti...  text        NaN        NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/796...                                     214001     67010          NaN          NaN\n",
      "4  16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...  text        NaN        NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/796...                                     178499     36688          NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "pandas.set_option('display.max_columns', None)  \n",
    "pandas.set_option('display.expand_frame_repr', False)\n",
    "pandas.set_option('display.precision', 3)\n",
    "df = pandas.read_csv('data.csv', sep=',', na_values=\"\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIMPIAR EL TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    today express deepest gratitud serv arm thankavet\n",
      "1    busi day plan new soon make import decis peopl...\n",
      "2    love fact small group protest last night passi...\n",
      "3      open success presidenti profession incit unfair\n",
      "4    fantast day met presid obama first realli good...\n",
      "Name: cleaned_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Preparación de las herramientas de preprocesamiento de texto\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english')) \n",
    "sno = SnowballStemmer('english') \n",
    "\n",
    "def cleanhtml(sentence): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "\n",
    "# Procesamiento del texto de los tweets\n",
    "final_string = []\n",
    "for sent in df['Tweet_Text'].values:\n",
    "    filtered_sentence = []\n",
    "    sent = cleanhtml(sent) \n",
    "    for w in sent.split():\n",
    "        cleaned_words = cleanpunc(w)\n",
    "        if cleaned_words.isalpha() and len(cleaned_words) > 2:\n",
    "            if cleaned_words.lower() not in stop:\n",
    "                stemmed_word = sno.stem(cleaned_words.lower())\n",
    "                filtered_sentence.append(stemmed_word)\n",
    "    final_string.append(\" \".join(filtered_sentence))\n",
    "\n",
    "# Añadiendo la columna de tweets limpios al DataFrame\n",
    "df['cleaned_tweet'] = final_string\n",
    "\n",
    "# Visualización de los primeros registros del DataFrame modificado\n",
    "print(df[\"cleaned_tweet\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREAR PORCENTAJE DE MUCHO Y POCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viral\n",
      "poco     3773\n",
      "mucho    3602\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "media_likes= df[\"twt_favourites_IS_THIS_LIKE_QUESTION_MARK\"].median()\n",
    "media_retweets=df[\"Retweets\"].median()\n",
    "df['Viral'] = df.apply(lambda x: 'mucho' if (x['twt_favourites_IS_THIS_LIKE_QUESTION_MARK'] + x['Retweets']) >= (media_likes + media_retweets) * 1.05\n",
    "                             else ('poco'), axis=1)\n",
    "print(df[\"Viral\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELIMINAR COLUMNAS INECESAREAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       cleaned_tweet  Viral\n",
      "0  today express deepest gratitud serv arm thankavet  mucho\n",
      "1  busi day plan new soon make import decis peopl...  mucho\n",
      "2  love fact small group protest last night passi...  mucho\n",
      "3    open success presidenti profession incit unfair  mucho\n",
      "4  fantast day met presid obama first realli good...  mucho\n"
     ]
    }
   ],
   "source": [
    "df.drop('Tweet_Text', axis=1, inplace=True)\n",
    "df.drop('Tweet_Id', axis=1, inplace=True)\n",
    "df.drop('Tweet_Url', axis=1, inplace=True)\n",
    "df.drop('Date', axis=1, inplace=True)\n",
    "df.drop('Time', axis=1, inplace=True)\n",
    "df.drop('Media_Type', axis=1, inplace=True)\n",
    "df.drop('Type', axis=1, inplace=True)\n",
    "df.drop('Hashtags', axis=1, inplace=True)\n",
    "df.drop('twt_favourites_IS_THIS_LIKE_QUESTION_MARK', axis=1, inplace=True)\n",
    "df.drop('Retweets', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 10', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 11', axis=1, inplace=True)\n",
    "y=df[\"Viral\"].values\n",
    "# Definir X como todas las columnas excepto 'Viral'\n",
    "X= df.drop('Viral', axis=1).values\n",
    "print(df[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAR COLUMNAS POR PALABRAS I FILTRAR POR UMBRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform( \u001b[43mX\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame(X\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mfeature_names)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform( X[:, -1])\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "X = pandas.DataFrame(X.toarray(), columns=feature_names)\n",
    "word_frequencies = X.sum(axis=0)\n",
    "filtered_words = word_frequencies[word_frequencies >= 5]\n",
    "X = X[filtered_words.index]\n",
    "X.to_csv('X.csv', index=False)\n",
    "df.drop('cleaned_tweet', axis=1, inplace=True)\n",
    "df.to_csv('y.csv', index=False)\n",
    "print(X[0:5])\n",
    "print(y[0:5])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
