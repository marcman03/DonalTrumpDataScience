{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1- Data Preprocessing:\n",
    "\n",
    "Assume that in data we have a dataframe where the column 'tweet' (data['tweet']) contains the text we want to convert into a table.\n",
    "\n",
    "In the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1) Begin by removing the html tags\n",
    "\n",
    "2) Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "\n",
    "3) Check if the word is made up of english letters and is not alpha-numeric\n",
    "\n",
    "4) Check to see if the length of the word is greater than 2 (as it is known that there is no adjective in 2-letters)\n",
    "\n",
    "5) Convert the word to lowercase\n",
    "\n",
    "6) Remove Stopwords (in nltk we have a list of stopwords (conjuntions, prepositions, articles) that appear in all kinds of documents and so they are not useful for discriminating between positive and negative reviews.\n",
    "\n",
    "7) Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming). Stemming consist in keeping the root of the word to represent toghether in a single column, for instance, different tenses of the same verb.\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 27 (1084714617.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 28\u001b[1;36m\u001b[0m\n\u001b[1;33m    filtered_sentence=[]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 27\n"
     ]
    }
   ],
   "source": [
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    stop = set(stopwords.words('english')) \n",
    "    sno = nltk.stem.SnowballStemmer('english') \n",
    "    \n",
    "    def cleanhtml(sentence): \n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, ' ', sentence)\n",
    "        return cleantext\n",
    "    def cleanpunc(sentence): \n",
    "        cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "        return  cleaned\n",
    "\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] \n",
    "all_negative_words=[] \n",
    "s=''\n",
    "for sent in data['tweet'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent) \n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    \n",
    "    str1 = b\" \".join(filtered_sentence) \n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "# A new column \"cleaned_tweet\" is added to dataset containing review after removing html tags, \n",
    "# punc, stopwords and after stemming and lematization\n",
    "\n",
    "data['cleaned_tweet']=final_string\n",
    "\n",
    "# See new dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 'cleaned_tweet' has the words to use and column 'tweet' can be discarded.\n",
    "You should apply split of data into training and test set. Assume 'X_train' and 'X_test' are the training ans test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize training: \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize test: \u001b[39m\u001b[38;5;124m'\u001b[39m,X_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m X_test\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Size training: ',X_train.shape)\n",
    "print('Size test: ',X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from text to Bag of Words representation as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Conversion from text to Bag of Words representation as a table\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      5\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m      6\u001b[0m Bow_train \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "## Conversion from text to Bag of Words representation as a table\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "Bow_train = count_vectorizer.fit_transform(X_train)\n",
    "Bow_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "print('Shape of table for training: ',Bow_train.shape)\n",
    "print('Shape of table for test: ',Bow_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
