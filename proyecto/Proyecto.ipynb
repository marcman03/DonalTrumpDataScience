{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pandas.set_option('display.max_columns', None)  \n",
    "pandas.set_option('display.expand_frame_repr', False)\n",
    "#pandas.set_option('max_colwidth', -1)\n",
    "pandas.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date      Time                                         Tweet_Text  Type Media_Type                    Hashtags   Tweet_Id                                          Tweet_Url  twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  Unnamed: 11\n",
      "0  16-11-11  15:26:37  Today we express our deepest gratitude to all ...  text      photo                   ThankAVet  7.970e+17  https://twitter.com/realDonaldTrump/status/797...                                     127213     41112          NaN          NaN\n",
      "1  16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...  text        NaN                         NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/797...                                     141527     28654          NaN          NaN\n",
      "2  16-11-11  11:14:20  Love the fact that the small groups of protest...  text        NaN                         NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/797...                                     183729     50039          NaN          NaN\n",
      "3  16-11-11   2:19:44  Just had a very open and successful presidenti...  text        NaN                         NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/796...                                     214001     67010          NaN          NaN\n",
      "4  16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...  text        NaN                         NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/796...                                     178499     36688          NaN          NaN\n",
      "5  16-11-10  19:31:27  Happy 241st birthday to the U.S. Marine Corps!...  text      photo                         NaN  7.970e+17  https://twitter.com/realDonaldTrump/status/796...                                     159176     44655          NaN          NaN\n",
      "6  16-11-09  11:36:58  Such a beautiful and important evening! The fo...  text        NaN                         NaN  7.960e+17  https://twitter.com/realDonaldTrump/status/796...                                     627615    225164          NaN          NaN\n",
      "7  16-11-09   2:48:27  Watching the returns at 9:45pm.\\n#ElectionNigh...  text      photo          ElectionNight;MAGA  7.960e+17  https://twitter.com/realDonaldTrump/status/796...                                     185160     45492          NaN          NaN\n",
      "8  16-11-09   1:35:15  RT @IvankaTrump: Such a surreal moment to vote...  text        NaN                         NaN  7.960e+17  https://twitter.com/realDonaldTrump/status/796...                                      99809     17169          NaN          NaN\n",
      "9  16-11-08  23:20:39  RT @EricTrump: Join my family in this incredib...  text        NaN  MakeAmericaGreatAgain;VOTE  7.960e+17  https://twitter.com/realDonaldTrump/status/796...                                      63868     19710          NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv('data.csv', sep=',', na_values=\"\")\n",
    "print(df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marcp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    today express deepest gratitud serv arm thankavet\n",
      "1    busi day plan new soon make import decis peopl...\n",
      "2    love fact small group protest last night passi...\n",
      "3      open success presidenti profession incit unfair\n",
      "4    fantast day met presid obama first realli good...\n",
      "Name: cleaned_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Preparación de las herramientas de preprocesamiento de texto\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english')) \n",
    "sno = SnowballStemmer('english') \n",
    "\n",
    "def cleanhtml(sentence): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "\n",
    "# Procesamiento del texto de los tweets\n",
    "final_string = []\n",
    "for sent in df['Tweet_Text'].values:\n",
    "    filtered_sentence = []\n",
    "    sent = cleanhtml(sent) \n",
    "    for w in sent.split():\n",
    "        cleaned_words = cleanpunc(w)\n",
    "        if cleaned_words.isalpha() and len(cleaned_words) > 2:\n",
    "            if cleaned_words.lower() not in stop:\n",
    "                stemmed_word = sno.stem(cleaned_words.lower())\n",
    "                filtered_sentence.append(stemmed_word)\n",
    "    final_string.append(\" \".join(filtered_sentence))\n",
    "\n",
    "# Añadiendo la columna de tweets limpios al DataFrame\n",
    "df['cleaned_tweet'] = final_string\n",
    "\n",
    "# Visualización de los primeros registros del DataFrame modificado\n",
    "print(df[\"cleaned_tweet\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "media_likes= df[\"twt_favourites_IS_THIS_LIKE_QUESTION_MARK\"].median()\n",
    "media_retweets=df[\"Retweets\"].median()\n",
    "df['Viral'] = df.apply(lambda x: \"mucho\" if (x['twt_favourites_IS_THIS_LIKE_QUESTION_MARK'] + x['Retweets']) >= (media_likes + media_retweets) * 3\n",
    "                       else (\"moderado\" if (x['twt_favourites_IS_THIS_LIKE_QUESTION_MARK'] + x['Retweets']) >= (media_likes + media_retweets)*0.5\n",
    "                             else \"poco\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viral\n",
      "moderado    3464\n",
      "poco        2272\n",
      "mucho       1639\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Viral\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Tweet_Text'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTweet_Text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Id\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Url\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Tweet_Text'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop('Tweet_Text', axis=1, inplace=True)\n",
    "df.drop('Tweet_Id', axis=1, inplace=True)\n",
    "df.drop('Tweet_Url', axis=1, inplace=True)\n",
    "df.drop('twt_favourites_IS_THIS_LIKE_QUESTION_MARK', axis=1, inplace=True)\n",
    "df.drop('Retweets', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 10', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 11', axis=1, inplace=True)\n",
    "y=df[\"Viral\"].values\n",
    "# Definir X como todas las columnas excepto 'Viral'\n",
    "X= df.drop('Viral', axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date      Time  Type Media_Type                    Hashtags                                      cleaned_tweet  Viral\n",
      "0  16-11-11  15:26:37  text      photo                   ThankAVet  today express deepest gratitud serv arm thankavet  mucho\n",
      "1  16-11-11  13:33:35  text        NaN                         NaN  busi day plan new soon make import decis peopl...  mucho\n",
      "2  16-11-11  11:14:20  text        NaN                         NaN  love fact small group protest last night passi...  mucho\n",
      "3  16-11-11   2:19:44  text        NaN                         NaN    open success presidenti profession incit unfair  mucho\n",
      "4  16-11-11   2:10:46  text        NaN                         NaN  fantast day met presid obama first realli good...  mucho\n",
      "5  16-11-10  19:31:27  text      photo                         NaN             happi birthday marin corp thank servic  mucho\n",
      "6  16-11-09  11:36:58  text        NaN                         NaN  beauti import even forgotten man woman never f...  mucho\n",
      "7  16-11-09   2:48:27  text      photo          ElectionNight;MAGA                         watch return electionnight  mucho\n",
      "8  16-11-09   1:35:15  text        NaN                         NaN  surreal moment vote father presid unit state m...  mucho\n",
      "9  16-11-08  23:20:39  text        NaN  MakeAmericaGreatAgain;VOTE  join famili incred movement makeamericagreatag...  mucho\n"
     ]
    }
   ],
   "source": [
    "print(df[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Simple cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training:  (5162, 6)\n",
      "Size test:  (2213, 6)\n",
      "[['16-04-25' '18:11:08' 'link' nan nan 'dead clinton trump']\n",
      " ['15-11-13' '20:18:12' 'text' nan nan\n",
      "  'said four week ago putinnev said green separ piecesgreat rate']\n",
      " ['16-02-17' '3:04:22' 'text' nan nan\n",
      "  'explain know beat hillari win state']\n",
      " ['16-01-15' '15:30:38' 'text' nan nan\n",
      "  'lindsey graham embarrass fail run presid embarrass endors']\n",
      " ['16-02-16' '11:22:02' 'text' nan nan\n",
      "  'new ppp poll trump rubio bush debat even stack rnc wonder']]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as cv \n",
    "(X_train, X_test,  y_train, y_test) = cv.train_test_split(X, y, test_size=.3, random_state=1)\n",
    "print('Size training: ',X_train.shape)\n",
    "print('Size test: ',X_test.shape)\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great honor result cnn poll believ final result even better'\n",
      " 'drug pour ice endors debat bigleaguetruth'\n",
      " 'michigan gop poll trump rubio cruz kasich carson wow' ...\n",
      " 'peopl tweet million low poll big hit ad'\n",
      " 'thank great crowd support get vote let maga'\n",
      " 'saw share want love hero cute']\n"
     ]
    }
   ],
   "source": [
    "print( X_train[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of table for training:  (5162, 4108)\n",
      "Shape of table for test:  (2213, 4108)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 84 stored elements and shape (10, 4108)>\n",
      "  Coords\tValues\n",
      "  (0, 1567)\t1\n",
      "  (0, 1710)\t1\n",
      "  (0, 2986)\t2\n",
      "  (0, 674)\t1\n",
      "  (0, 2719)\t1\n",
      "  (0, 317)\t1\n",
      "  (0, 1356)\t1\n",
      "  (0, 1232)\t1\n",
      "  (0, 345)\t1\n",
      "  (1, 1099)\t1\n",
      "  (1, 2739)\t1\n",
      "  (1, 1754)\t1\n",
      "  (1, 1181)\t1\n",
      "  (1, 899)\t1\n",
      "  (1, 354)\t1\n",
      "  (2, 2719)\t1\n",
      "  (2, 2271)\t1\n",
      "  (2, 1532)\t1\n",
      "  (2, 3727)\t1\n",
      "  (2, 3065)\t1\n",
      "  (2, 854)\t1\n",
      "  (2, 1962)\t1\n",
      "  (2, 547)\t1\n",
      "  (2, 4058)\t1\n",
      "  (3, 3065)\t1\n",
      "  :\t:\n",
      "  (7, 490)\t1\n",
      "  (7, 2104)\t1\n",
      "  (7, 481)\t1\n",
      "  (7, 340)\t1\n",
      "  (7, 2893)\t1\n",
      "  (7, 1224)\t1\n",
      "  (7, 211)\t1\n",
      "  (7, 113)\t1\n",
      "  (8, 2634)\t1\n",
      "  (8, 2053)\t1\n",
      "  (8, 2057)\t1\n",
      "  (8, 380)\t1\n",
      "  (8, 351)\t1\n",
      "  (8, 3650)\t1\n",
      "  (9, 1567)\t1\n",
      "  (9, 2719)\t1\n",
      "  (9, 3727)\t1\n",
      "  (9, 113)\t1\n",
      "  (9, 510)\t1\n",
      "  (9, 2470)\t1\n",
      "  (9, 3947)\t1\n",
      "  (9, 2174)\t1\n",
      "  (9, 118)\t1\n",
      "  (9, 2707)\t1\n",
      "  (9, 2045)\t1\n"
     ]
    }
   ],
   "source": [
    "## Conversion from text to Bag of Words representation as a table\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "Bow_train = count_vectorizer.fit_transform( X_train[:, -1])\n",
    "Bow_test = count_vectorizer.transform( X_test[:, -1])\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print('Shape of table for training: ',Bow_train.shape)\n",
    "print('Shape of table for test: ',Bow_test.shape)\n",
    "print( Bow_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_train = pandas.DataFrame(Bow_train.toarray(), columns=feature_names)\n",
    "df_bow_train.to_csv('bow_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47401717126073206\n"
     ]
    }
   ],
   "source": [
    "import sklearn                         # Llibreia de DM\n",
    "import sklearn.datasets as ds            # Per carregar mÃ©s facilment el dataset digits\n",
    "import sklearn.model_selection as cv    # Pel Cross-validation\n",
    "import sklearn.neighbors as nb \n",
    "knc = nb.KNeighborsClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "knc.fit(Bow_train, y_train)\n",
    "\n",
    "# Obtain accuracy score of learned classifier on test data\n",
    "print(knc.score(Bow_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[630  46 380]\n",
      " [273  64 147]\n",
      " [302  16 355]]\n"
     ]
    }
   ],
   "source": [
    "# More4 information with confussion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = knc.predict(Bow_test)\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    moderado       0.52      0.60      0.56      1056\n",
      "       mucho       0.51      0.13      0.21       484\n",
      "        poco       0.40      0.53      0.46       673\n",
      "\n",
      "    accuracy                           0.47      2213\n",
      "   macro avg       0.48      0.42      0.41      2213\n",
      "weighted avg       0.48      0.47      0.45      2213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain Recall, Precision and F-Measure for each class\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
