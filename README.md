# ğŸ“Š PredicciÃ³n de Viralidad de Tweets de Donald Trump

Este proyecto analiza tweets de Donald Trump (2015â€“2016) con el objetivo de predecir si un tweet serÃ¡ **viral** (â€œmuchoâ€) o **no viral** (â€œpocoâ€) usando tÃ©cnicas de minerÃ­a de datos y modelos de machine learning.

---

## ğŸ—‚ï¸ Dataset

- Fuente: Kaggle (Donald Trump Tweets Dataset).  
- 8.716 tweets y 11 columnas.  
- Columnas Ãºtiles para el modelo:
  - `Tweet_Text`
  - `Likes`
  - `Retweets`

Se eliminan columnas irrelevantes o vacÃ­as (`Media_Type`, `Hashtags`, `Unnamed:*`, etc.).

---

## ğŸ§¹ Preprocesamiento

1. Limpieza del texto:
   - EliminaciÃ³n de HTML, puntuaciÃ³n y *stopwords*.
   - LematizaciÃ³n/stemming.
   - EliminaciÃ³n de palabras cortas y no alfabÃ©ticas.

2. VectorizaciÃ³n:
   - `CountVectorizer (min_df = 5)`
   - Resultado: **7375 tweets Ã— 1477 palabras**

3. DefiniciÃ³n del target:
   - Viral = interacciÃ³n â‰¥ 1.05 Ã— (mediana likes + mediana retweets)
   - Dataset balanceado:
     - mucho: 3602  
     - poco: 3773  

---

## ğŸ“ EvaluaciÃ³n

- DivisiÃ³n estratificada **70% train â€“ 30% test**
- **10-Fold Cross Validation**
- MÃ©tricas:
  - accuracy  
  - precision  
  - recall  
  - F1-score  
  - matriz de confusiÃ³n  
- CÃ¡lculo de intervalos de confianza al 95%

---

## ğŸ¤– Modelos Probados

### ğŸŸ¦ NaÃ¯ve Bayes (MultinomialNB)
- Mejor rendimiento y mayor estabilidad  
- Umbral optimizado â‰ˆ 0.236  
- **Accuracy:** ~72%  
- **IC 95%:** (0.696, 0.733)

### ğŸŸ© KNN
- Mejor con `weights='distance'` y `SelectKBest`  
- **Accuracy:** ~68%  
- **IC 95%:** (0.662, 0.701)

### ğŸŸ§ Decision Tree
- Criterio entropy, ajuste de impureza mÃ­nima  
- **Accuracy:** ~67%  
- **IC 95%:** (0.657, 0.696)

### ğŸŸ¥ SVM
- Kernels probados: lineal, polinomial, RBF  
- Mejor: **kernel lineal (~64%)**  
- **IC 95%:** (0.509, 0.657)

### ğŸŸª Meta-Learning (Ensembles)
- MÃ©todos: Voting, Bagging, Random Forest, AdaBoost  
- Mejor ensemble: **Random Forest (~71%)**  
- **IC 95% RF:** (0.636, 0.658)

---

## ğŸ Conclusiones

- **Mejor modelo global:** NaÃ¯ve Bayes  
- **Mejor ensemble:** Random Forest  
- **Modelo mÃ¡s interpretable:** Decision Tree  
- SVM y KNN funcionan, pero con precisiÃ³n inferior o problemas de escalabilidad  
- Los modelos probabilÃ­sticos y ensembles funcionan mejor en texto de alta dimensionalidad  

---


---

## â–¶ï¸ EjecuciÃ³n

```bash
pip install -r requirements.txt
python src/preprocessing.py
python src/models/naive_bayes.py

